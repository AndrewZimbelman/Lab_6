[
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(parsnip)\necho = FALSE\nroot &lt;-'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf',\n              'camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('{root}/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_clim.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_clim.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_geol.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_geol.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_soil.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_soil.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_topo.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_topo.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_vege.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_vege.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt: cannot open\ndestfile 'https://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt', reason\n'Invalid argument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "docs/index.html#question-1",
    "href": "docs/index.html#question-1",
    "title": "",
    "section": "Question 1",
    "text": "Question 1\nit appears as if zero q mean is meant to represent series of dates with 0 mean daily discharge"
  },
  {
    "objectID": "docs/index.html#question-2",
    "href": "docs/index.html#question-2",
    "title": "",
    "section": "Question 2",
    "text": "Question 2\nGraph 1\n\n#graph 1 \ncamels_aridity &lt;- camels |&gt; \n  arrange(-aridity)\naridity_graph &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n      borders(\"state\", colour = \"grey50\") + \n      geom_point(size = 0.5,aes(color = p_mean)) +\n      scale_color_gradient(low = \"blue\", high = \"yellow\") + \n      theme_linedraw() +\n      labs(  title = \"Precipitation & Aridity In United States River Basins\", x = \"Latitude\", y = \"Longitude\", color = 'aridity')\n\n      \n\n# Color scale was determined measuring from low aridty which would imply a less dry environment compared to a high aridity environment which would imply a dry environment. \n\nGraph 2\n\ncamels_aridity &lt;- camels |&gt; \n  arrange(-aridity)\npmean_graph &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +                             borders(\"state\", colour = \"grey50\") + \n     geom_point(size = 0.5,aes(color = p_mean)) +\n     scale_color_gradient(low = \"yellow\", high = \"blue\") + \n    theme_linedraw() +\n     labs( x = \"Latitude\", y = \"Longitude\", scale = \"free\", color = \"Mean Precipitation \")\n\nGraphs Combined\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(gridExtra)\n\nWarning: package 'gridExtra' was built under R version 4.4.3\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\naridity_graph + pmean_graph"
  },
  {
    "objectID": "docs/index.html#question-3",
    "href": "docs/index.html#question-3",
    "title": "",
    "section": "Question 3",
    "text": "Question 3\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\ncamels &lt;- camels |&gt;\n  mutate(LogQmean = log(q_mean))\nset.seed(10)\n  camels_split &lt;- initial_split(camels, prop = 0.8)\n  camels_training &lt;- training(camels_split)\n  camels_test &lt;- testing(camels_split)\n  camel_fold &lt;- vfold_cv(camels_training, v = 10)\n\nrec &lt;- recipe(LogQmean ~ p_mean + aridity, data = camels_training) |&gt;\n  step_log(all_predictors())|&gt;\n  step_interact(terms = ~ p_mean:aridity)|&gt;\n  step_naomit(all_predictors(), all_outcomes())\n  \nxg_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\")|&gt;\n  set_mode(\"regression\")\n\nxg_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(xg_model)|&gt;\n  fit(data = camels_training)\nxg_data &lt;- augment(xg_wf, new_data = camels_test)\n\nnueral_model &lt;- bag_mlp()|&gt;\n  set_engine(\"nnet\")|&gt;\n  set_mode(\"regression\")\nnueral_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(nueral_model)|&gt;\n  fit(data = camels_training)\nnueral_data &lt;- augment(nueral_wf, new_data = camels_test)\nwf &lt;- workflow_set(list(rec), list(xg_model, nueral_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camel_fold) \n\nautoplot(wf)\n\n\n\n\n\n\n\n  rank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_boost_tree Prepro… rmse    0.622  0.0266    10 recipe       boos…     1\n2 recipe_boost_tree Prepro… rsq     0.723  0.0233    10 recipe       boos…     1\n3 recipe_bag_mlp    Prepro… rmse    0.670  0.0620    10 recipe       bag_…     2\n4 recipe_bag_mlp    Prepro… rsq     0.716  0.0328    10 recipe       bag_…     2\n\n\nOverall, The xgboost models seem to work better, as they have a significantly lower standard error value then the nueral network models, they also rank higher in rsq values which tells us more of the variance in the model can be explained by the data."
  },
  {
    "objectID": "docs/index.html#question-4",
    "href": "docs/index.html#question-4",
    "title": "",
    "section": "Question 4",
    "text": "Question 4\nChoosing Data\n\n# I'm going to select terms that mean daily discharge should have a significant relationship with and use correlation models to see which ones I could possibly model\n camels |&gt;\n   select(q_mean,slope_mean,slope_fdc, area_gages2,frac_snow,high_prec_freq,low_prec_freq)|&gt;\n  drop_na()|&gt;\n  cor()\n\n                   q_mean slope_mean   slope_fdc area_gages2   frac_snow\nq_mean          1.0000000  0.5151069  0.48559554 -0.15561081  0.12368627\nslope_mean      0.5151069  1.0000000  0.16887572 -0.13825569  0.63212351\nslope_fdc       0.4855955  0.1688757  1.00000000 -0.06269347 -0.01881482\narea_gages2    -0.1556108 -0.1382557 -0.06269347  1.00000000 -0.03519798\nfrac_snow       0.1236863  0.6321235 -0.01881482 -0.03519798  1.00000000\nhigh_prec_freq -0.6687589 -0.4714672 -0.45027411  0.02920471 -0.30961233\nlow_prec_freq  -0.7145711 -0.2949111 -0.61857492  0.08251738 -0.19895077\n               high_prec_freq low_prec_freq\nq_mean            -0.66875889   -0.71457114\nslope_mean        -0.47146724   -0.29491106\nslope_fdc         -0.45027411   -0.61857492\narea_gages2        0.02920471    0.08251738\nfrac_snow         -0.30961233   -0.19895077\nhigh_prec_freq     1.00000000    0.87176299\nlow_prec_freq      0.87176299    1.00000000\n\n#Strong Correlation is observed with days per year with precipitation 5x higher then mean daily precipitation as well as dry days per year. model will be built to determine if we can predict mean flow based on a certain number of dry days as well as high precipitation days per year. Possibly to help with further research on how climate change could impact river basin mean flow. \ncamels &lt;- camels |&gt;\n  mutate(log_high_freq = log(high_prec_freq))\n\n\n#Code to Visualize distribution of data \nlow_prec_freq &lt;- camels |&gt;\n  pull(low_prec_freq)\nhigh_prec_freq &lt;- camels |&gt;\n  pull(high_prec_freq)\nlog_prec_high &lt;- camels |&gt;\n  mutate(log_high_prec = log(high_prec_freq))|&gt;\n  pull(log_high_prec)\ngghistogram(binwidth=2,low_prec_freq, main = \"Dry Days\")\n\n\n\n\n\n\n\n#low_prec_freq data looks relativley normally distributed \ngghistogram(binwidth = 0.8, high_prec_freq, main = \"High Rain Days\")\n\n\n\n\n\n\n\n#high_prec_freq data is slightly left skewed, log makes this skew worse. We will use the non log version of our data for this model. \ngghistogram(binwidth = 0.1, high_prec_freq, main = \"High Rain Days\")\n\n\n\n\n\n\n\nHigh_flow_graph &lt;- camels |&gt;\n  ggplot(aes(x = high_prec_freq, y = q_mean,color = \"q_mean\")) +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"blue\", high = \"yellow\") +\n  labs(x = \"High Precipitation Days Per year\",\n       y = \"Mean Daily Discharge\",\n       col = \"Daily Discharge\")\nLow_flow_graph &lt;- camels |&gt;\n  ggplot(aes(x = low_prec_freq, y = q_mean, color = \"q_mean\")) + \n  geom_point(aes(color = q_mean)) + \n  scale_color_gradient(low = 'blue', high = 'yellow') +\n  labs(x = \"Dry Days Per Year\",\n       y = \"Mean Daily Discharge\",\n       col = \"Daily Discharge\")\nLow_flow_graph\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nHigh_flow_graph\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nRegression & Recipe making\n\n#Linear regression will be used to determine if the dry days and high precipitation days data both have a significant relationship with q_mean. If so we will use both if not we will use the one that does for our model\nlinear_model = lm(LogQmean ~ high_prec_freq + low_prec_freq, data = camels)\nsummary(linear_model)\n\n\nCall:\nlm(formula = LogQmean ~ high_prec_freq + low_prec_freq, data = camels)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9132 -0.3929  0.0574  0.4442  2.4467 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     6.848565   0.232702  29.431  &lt; 2e-16 ***\nhigh_prec_freq  0.051133   0.012874   3.972 7.91e-05 ***\nlow_prec_freq  -0.031513   0.001666 -18.910  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.742 on 667 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.6022,    Adjusted R-squared:  0.601 \nF-statistic: 504.9 on 2 and 667 DF,  p-value: &lt; 2.2e-16\n\n#It appears both have a p value less than 0.05 suggesting that this data has a significant relationship, we will use the data but we will need to try some stuff to get a higher r squared value in our final model. \nset.seed(123)\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_fold &lt;- vfold_cv(camels_train, v = 10)\nrec &lt;- recipe(LogQmean ~ high_prec_freq + low_prec_freq, data = camels_train)|&gt;\n  step_BoxCox(all_predictors()) |&gt;\n  step_center(high_prec_freq)|&gt;\n  #These steps should help raise the r squared value\n  step_naomit(all_predictors(),all_outcomes())\nbaked_data =  prep(rec)|&gt;\n  bake(new_data = NULL)\n#The formula used for this was mainly centered around the desire to remove the left skew I had in my predictor values data, primarily in high_prec_freq. Because of this, I used BoxCox which helps to make data have a more normal dsitrubtion as well as center which should also help with this\n\nsummary(lm(LogQmean ~ high_prec_freq + low_prec_freq, data = baked_data))\n\n\nCall:\nlm(formula = LogQmean ~ high_prec_freq + low_prec_freq, data = baked_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9315 -0.3829  0.0725  0.4368  2.2228 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.2534744  0.3511128  14.962   &lt;2e-16 ***\nhigh_prec_freq  0.0006937  0.0004553   1.524    0.128    \nlow_prec_freq  -0.0040027  0.0002604 -15.370   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7604 on 499 degrees of freedom\nMultiple R-squared:  0.5949,    Adjusted R-squared:  0.5932 \nF-statistic: 366.3 on 2 and 499 DF,  p-value: &lt; 2.2e-16\n\n#r squared is around 59% which isn't great but it will be servicable for our models \n\n\nrand_model &lt;- rand_forest()|&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\nrand_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(rand_model)|&gt;\n  fit(data = camels_train)\nBoost_model &lt;- boost_tree()|&gt;\n  set_engine(\"xgboost\")|&gt;\n  set_mode(\"regression\")\nlog_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(Boost_model)|&gt;\n  fit(data = camels_train)\nlin_model &lt;- linear_reg() |&gt;\nset_engine(\"lm\") |&gt;\nset_mode(\"regression\")\nlin_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(lin_model) |&gt;\n  fit(data = camels_train)\n\nworkflow set & evaluation\n\nwf &lt;- workflow_set(list(rec), list(lin_model,Boost_model,rand_model))|&gt;\n      workflow_map('fit_resamples', resamples = camels_fold)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.715  0.0395    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.650  0.0313    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.740  0.0457    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.629  0.0327    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.761  0.0292    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.601  0.0217    10 recipe       line…     3\n\n#Our results show that our models haven't been very succesful at showing a relationship and may not be the best as predictors, the best model to use is our rand_forest model so we will use this one and evaluate later. \n\nModel Prediction & Evaluation\n\npred_graph &lt;-augment(rand_wf, new_data = camels_test)\nmetrics(pred_graph, truth = LogQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.636\n2 rsq     standard       0.682\n3 mae     standard       0.451\n\nobs_pred &lt;- ggplot(pred_graph, aes(x = LogQmean, y =.pred, color = LogQmean))+\n  scale_color_viridis_c() + \n  geom_point()+\n  labs( x = \"Mean Daily Discharge\", y = \"Predicted Values\",\n        color = \"Mean Discharge\", title = \"Predicted vs             Observed \") +\n        geom_abline()\nobs_pred\n\n\n\n\n\n\n\n\nOverall, our predicted values aligned with our observed values where the majority of the data was centralized which tells me that our values at the beginning and near the end of the graph were outliers from the data, . I believe the r squared value would have increased and a better model would have been made if outliers were eliminated at the beginning of the model creation by filtering out values greater then a certain value, or replacing them with the mean value of the datset. . Overall, this was my first model that I’ve made and I feel like I have a better understanding about how model building works and I look forward to getting better at it as time goes on and better understanding the different models I can use. This lab was fun!"
  },
  {
    "objectID": "docs/lab6.html",
    "href": "docs/lab6.html",
    "title": "",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(parsnip)\necho = FALSE\nroot &lt;-'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf',\n              'camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('{root}/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_clim.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_clim.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_geol.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_geol.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_soil.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_soil.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_topo.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_topo.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_vege.txt: cannot open destfile\n'https://gdex.ucar.edu/dataset/camels/file/camels_vege.txt', reason 'Invalid\nargument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): URL\nhttps://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt: cannot open\ndestfile 'https://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt', reason\n'Invalid argument'\n\n\nWarning in .f(.x[[i]], .y[[i]], ...): download had nonzero exit status\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "docs/lab6.html#question-1",
    "href": "docs/lab6.html#question-1",
    "title": "",
    "section": "Question 1",
    "text": "Question 1\nit appears as if zero q mean is meant to represent series of dates with 0 mean daily discharge"
  },
  {
    "objectID": "docs/lab6.html#question-2",
    "href": "docs/lab6.html#question-2",
    "title": "",
    "section": "Question 2",
    "text": "Question 2\nGraph 1\n\n#graph 1 \ncamels_aridity &lt;- camels |&gt; \n  arrange(-aridity)\naridity_graph &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n      borders(\"state\", colour = \"grey50\") + \n      geom_point(size = 0.5,aes(color = p_mean)) +\n      scale_color_gradient(low = \"blue\", high = \"yellow\") + \n      theme_linedraw() +\n      labs(  title = \"Precipitation & Aridity In United States River Basins\", x = \"Latitude\", y = \"Longitude\", color = 'aridity')\n\n      \n\n# Color scale was determined measuring from low aridty which would imply a less dry environment compared to a high aridity environment which would imply a dry environment. \n\nGraph 2\n\ncamels_aridity &lt;- camels |&gt; \n  arrange(-aridity)\npmean_graph &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +                             borders(\"state\", colour = \"grey50\") + \n     geom_point(size = 0.5,aes(color = p_mean)) +\n     scale_color_gradient(low = \"yellow\", high = \"blue\") + \n    theme_linedraw() +\n     labs( x = \"Latitude\", y = \"Longitude\", scale = \"free\", color = \"Mean Precipitation \")\n\nGraphs Combined\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(gridExtra)\n\nWarning: package 'gridExtra' was built under R version 4.4.3\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\naridity_graph + pmean_graph"
  },
  {
    "objectID": "docs/lab6.html#question-3",
    "href": "docs/lab6.html#question-3",
    "title": "",
    "section": "Question 3",
    "text": "Question 3\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\ncamels &lt;- camels |&gt;\n  mutate(LogQmean = log(q_mean))\nset.seed(10)\n  camels_split &lt;- initial_split(camels, prop = 0.8)\n  camels_training &lt;- training(camels_split)\n  camels_test &lt;- testing(camels_split)\n  camel_fold &lt;- vfold_cv(camels_training, v = 10)\n\nrec &lt;- recipe(LogQmean ~ p_mean + aridity, data = camels_training) |&gt;\n  step_log(all_predictors())|&gt;\n  step_interact(terms = ~ p_mean:aridity)|&gt;\n  step_naomit(all_predictors(), all_outcomes())\n  \nxg_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\")|&gt;\n  set_mode(\"regression\")\n\nxg_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(xg_model)|&gt;\n  fit(data = camels_training)\nxg_data &lt;- augment(xg_wf, new_data = camels_test)\n\nnueral_model &lt;- bag_mlp()|&gt;\n  set_engine(\"nnet\")|&gt;\n  set_mode(\"regression\")\nnueral_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(nueral_model)|&gt;\n  fit(data = camels_training)\nnueral_data &lt;- augment(nueral_wf, new_data = camels_test)\nwf &lt;- workflow_set(list(rec), list(xg_model, nueral_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camel_fold) \n\nautoplot(wf)\n\n\n\n\n\n\n\n  rank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_boost_tree Prepro… rmse    0.622  0.0266    10 recipe       boos…     1\n2 recipe_boost_tree Prepro… rsq     0.723  0.0233    10 recipe       boos…     1\n3 recipe_bag_mlp    Prepro… rmse    0.670  0.0620    10 recipe       bag_…     2\n4 recipe_bag_mlp    Prepro… rsq     0.716  0.0328    10 recipe       bag_…     2\n\n\nOverall, The xgboost models seem to work better, as they have a significantly lower standard error value then the nueral network models, they also rank higher in rsq values which tells us more of the variance in the model can be explained by the data."
  },
  {
    "objectID": "docs/lab6.html#question-4",
    "href": "docs/lab6.html#question-4",
    "title": "",
    "section": "Question 4",
    "text": "Question 4\nChoosing Data\n\n# I'm going to select terms that mean daily discharge should have a significant relationship with and use correlation models to see which ones I could possibly model\n camels |&gt;\n   select(q_mean,slope_mean,slope_fdc, area_gages2,frac_snow,high_prec_freq,low_prec_freq)|&gt;\n  drop_na()|&gt;\n  cor()\n\n                   q_mean slope_mean   slope_fdc area_gages2   frac_snow\nq_mean          1.0000000  0.5151069  0.48559554 -0.15561081  0.12368627\nslope_mean      0.5151069  1.0000000  0.16887572 -0.13825569  0.63212351\nslope_fdc       0.4855955  0.1688757  1.00000000 -0.06269347 -0.01881482\narea_gages2    -0.1556108 -0.1382557 -0.06269347  1.00000000 -0.03519798\nfrac_snow       0.1236863  0.6321235 -0.01881482 -0.03519798  1.00000000\nhigh_prec_freq -0.6687589 -0.4714672 -0.45027411  0.02920471 -0.30961233\nlow_prec_freq  -0.7145711 -0.2949111 -0.61857492  0.08251738 -0.19895077\n               high_prec_freq low_prec_freq\nq_mean            -0.66875889   -0.71457114\nslope_mean        -0.47146724   -0.29491106\nslope_fdc         -0.45027411   -0.61857492\narea_gages2        0.02920471    0.08251738\nfrac_snow         -0.30961233   -0.19895077\nhigh_prec_freq     1.00000000    0.87176299\nlow_prec_freq      0.87176299    1.00000000\n\n#Strong Correlation is observed with days per year with precipitation 5x higher then mean daily precipitation as well as dry days per year. model will be built to determine if we can predict mean flow based on a certain number of dry days as well as high precipitation days per year. Possibly to help with further research on how climate change could impact river basin mean flow. \ncamels &lt;- camels |&gt;\n  mutate(log_high_freq = log(high_prec_freq))\n\n\n#Code to Visualize distribution of data \nlow_prec_freq &lt;- camels |&gt;\n  pull(low_prec_freq)\nhigh_prec_freq &lt;- camels |&gt;\n  pull(high_prec_freq)\nlog_prec_high &lt;- camels |&gt;\n  mutate(log_high_prec = log(high_prec_freq))|&gt;\n  pull(log_high_prec)\ngghistogram(binwidth=2,low_prec_freq, main = \"Dry Days\")\n\n\n\n\n\n\n\n#low_prec_freq data looks relativley normally distributed \ngghistogram(binwidth = 0.8, high_prec_freq, main = \"High Rain Days\")\n\n\n\n\n\n\n\n#high_prec_freq data is slightly left skewed, log makes this skew worse. We will use the non log version of our data for this model. \ngghistogram(binwidth = 0.1, high_prec_freq, main = \"High Rain Days\")\n\n\n\n\n\n\n\nHigh_flow_graph &lt;- camels |&gt;\n  ggplot(aes(x = high_prec_freq, y = q_mean,color = \"q_mean\")) +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"blue\", high = \"yellow\") +\n  labs(x = \"High Precipitation Days Per year\",\n       y = \"Mean Daily Discharge\",\n       col = \"Daily Discharge\")\nLow_flow_graph &lt;- camels |&gt;\n  ggplot(aes(x = low_prec_freq, y = q_mean, color = \"q_mean\")) + \n  geom_point(aes(color = q_mean)) + \n  scale_color_gradient(low = 'blue', high = 'yellow') +\n  labs(x = \"Dry Days Per Year\",\n       y = \"Mean Daily Discharge\",\n       col = \"Daily Discharge\")\nLow_flow_graph\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nHigh_flow_graph\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nRegression & Recipe making\n\n#Linear regression will be used to determine if the dry days and high precipitation days data both have a significant relationship with q_mean. If so we will use both if not we will use the one that does for our model\nlinear_model = lm(LogQmean ~ high_prec_freq + low_prec_freq, data = camels)\nsummary(linear_model)\n\n\nCall:\nlm(formula = LogQmean ~ high_prec_freq + low_prec_freq, data = camels)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9132 -0.3929  0.0574  0.4442  2.4467 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     6.848565   0.232702  29.431  &lt; 2e-16 ***\nhigh_prec_freq  0.051133   0.012874   3.972 7.91e-05 ***\nlow_prec_freq  -0.031513   0.001666 -18.910  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.742 on 667 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.6022,    Adjusted R-squared:  0.601 \nF-statistic: 504.9 on 2 and 667 DF,  p-value: &lt; 2.2e-16\n\n#It appears both have a p value less than 0.05 suggesting that this data has a significant relationship, we will use the data but we will need to try some stuff to get a higher r squared value in our final model. \nset.seed(123)\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_fold &lt;- vfold_cv(camels_train, v = 10)\nrec &lt;- recipe(LogQmean ~ high_prec_freq + low_prec_freq, data = camels_train)|&gt;\n  step_BoxCox(all_predictors()) |&gt;\n  step_center(high_prec_freq)|&gt;\n  #These steps should help raise the r squared value\n  step_naomit(all_predictors(),all_outcomes())\nbaked_data =  prep(rec)|&gt;\n  bake(new_data = NULL)\n#The formula used for this was mainly centered around the desire to remove the left skew I had in my predictor values data, primarily in high_prec_freq. Because of this, I used BoxCox which helps to make data have a more normal dsitrubtion as well as center which should also help with this\n\nsummary(lm(LogQmean ~ high_prec_freq + low_prec_freq, data = baked_data))\n\n\nCall:\nlm(formula = LogQmean ~ high_prec_freq + low_prec_freq, data = baked_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9315 -0.3829  0.0725  0.4368  2.2228 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.2534744  0.3511128  14.962   &lt;2e-16 ***\nhigh_prec_freq  0.0006937  0.0004553   1.524    0.128    \nlow_prec_freq  -0.0040027  0.0002604 -15.370   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7604 on 499 degrees of freedom\nMultiple R-squared:  0.5949,    Adjusted R-squared:  0.5932 \nF-statistic: 366.3 on 2 and 499 DF,  p-value: &lt; 2.2e-16\n\n#r squared is around 59% which isn't great but it will be servicable for our models \n\nModel Creation\n\nrand_model &lt;- rand_forest()|&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\nrand_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(rand_model)|&gt;\n  fit(data = camels_train)\nBoost_model &lt;- boost_tree()|&gt;\n  set_engine(\"xgboost\")|&gt;\n  set_mode(\"regression\")\nlog_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(Boost_model)|&gt;\n  fit(data = camels_train)\nlin_model &lt;- linear_reg() |&gt;\nset_engine(\"lm\") |&gt;\nset_mode(\"regression\")\nlin_wf &lt;- workflow()|&gt;\n  add_recipe(rec)|&gt;\n  add_model(lin_model) |&gt;\n  fit(data = camels_train)\n\nworkflow set & evaluation\n\nwf &lt;- workflow_set(list(rec), list(lin_model,Boost_model,rand_model))|&gt;\n      workflow_map('fit_resamples', resamples = camels_fold)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.715  0.0395    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.650  0.0313    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.740  0.0457    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.629  0.0327    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.761  0.0292    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.601  0.0217    10 recipe       line…     3\n\n#Our results show that our models haven't been very succesful at showing a relationship and may not be the best as predictors, the best model to use is our rand_forest model so we will use this one and evaluate later. \n\nModel Prediction & Evaluation\n\npred_graph &lt;-augment(rand_wf, new_data = camels_test)\nmetrics(pred_graph, truth = LogQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.636\n2 rsq     standard       0.682\n3 mae     standard       0.451\n\nobs_pred &lt;- ggplot(pred_graph, aes(x = LogQmean, y =.pred, color = LogQmean))+\n  scale_color_viridis_c() + \n  geom_point()+\n  labs( x = \"Mean Daily Discharge\", y = \"Predicted Values\",\n        color = \"Mean Discharge\", title = \"Predicted vs             Observed \") +\n        geom_abline()\nobs_pred\n\n\n\n\n\n\n\n\nOverall, our predicted values aligned with our observed values where the majority of the data was centralized which tells me that our values at the beginning and near the end of the graph were outliers from the data, . I believe the r squared value would have increased and a better model would have been made if outliers were eliminated at the beginning of the model creation by filtering out values greater then a certain value, or replacing them with the mean value of the datset. . Overall, this was my first model that I’ve made and I feel like I have a better understanding about how model building works and I look forward to getting better at it as time goes on and better understanding the different models I can use. This lab was fun!"
  }
]